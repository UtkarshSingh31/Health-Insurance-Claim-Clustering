{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32eb9004",
   "metadata": {},
   "source": [
    "### Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "852f1ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.metrics import (\n",
    "    silhouette_score, calinski_harabasz_score, davies_bouldin_score,\n",
    "    confusion_matrix, roc_auc_score,average_precision_score,\n",
    "    precision_score, recall_score, f1_score\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0e09dda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claim_id</th>\n",
       "      <th>claim_amount</th>\n",
       "      <th>claim_type</th>\n",
       "      <th>member_age</th>\n",
       "      <th>chronic_conditions_count</th>\n",
       "      <th>length_of_stay_days</th>\n",
       "      <th>num_procedures</th>\n",
       "      <th>procedure_category</th>\n",
       "      <th>provider_specialty</th>\n",
       "      <th>days_since_policy_start</th>\n",
       "      <th>weekend_claim_flag</th>\n",
       "      <th>multiple_claims_same_day</th>\n",
       "      <th>amount_per_day_of_stay</th>\n",
       "      <th>cost_per_procedure</th>\n",
       "      <th>high_amount_flag</th>\n",
       "      <th>high_cost_per_procedure</th>\n",
       "      <th>rushed_claim</th>\n",
       "      <th>is_fraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CLM100000</td>\n",
       "      <td>2446.03</td>\n",
       "      <td>outpatient</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>consultation</td>\n",
       "      <td>general</td>\n",
       "      <td>705</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2446.03</td>\n",
       "      <td>2446.03</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CLM100001</td>\n",
       "      <td>57383.85</td>\n",
       "      <td>hospitalization</td>\n",
       "      <td>66</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>surgery</td>\n",
       "      <td>cardiology</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5216.71</td>\n",
       "      <td>5738.38</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CLM100002</td>\n",
       "      <td>9185.75</td>\n",
       "      <td>dental</td>\n",
       "      <td>53</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>surgery</td>\n",
       "      <td>general</td>\n",
       "      <td>449</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1837.15</td>\n",
       "      <td>4592.88</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CLM100003</td>\n",
       "      <td>7140.03</td>\n",
       "      <td>outpatient</td>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>consultation</td>\n",
       "      <td>orthopedics</td>\n",
       "      <td>497</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2380.01</td>\n",
       "      <td>3570.02</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CLM100004</td>\n",
       "      <td>2277.99</td>\n",
       "      <td>outpatient</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>consultation</td>\n",
       "      <td>general</td>\n",
       "      <td>604</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2277.99</td>\n",
       "      <td>2277.99</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    claim_id  claim_amount  ... rushed_claim  is_fraud\n",
       "0  CLM100000       2446.03  ...            0         0\n",
       "1  CLM100001      57383.85  ...            0         1\n",
       "2  CLM100002       9185.75  ...            0         0\n",
       "3  CLM100003       7140.03  ...            0         0\n",
       "4  CLM100004       2277.99  ...            0         0\n",
       "\n",
       "[5 rows x 18 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(\"E:\\\\Fraud Detection\\\\data\\\\train_data\\\\healthcare_fraud.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0cfd806d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features for modeling: 16\n"
     ]
    }
   ],
   "source": [
    "id_cols = ['claim_id']\n",
    "label_cols = ['is_fraud']\n",
    "exclude_for_training = id_cols + label_cols\n",
    "\n",
    "feature_df = df.drop(columns=exclude_for_training)\n",
    "\n",
    "print(f\"Features for modeling: {feature_df.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8490f7c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical features length: 3\n",
      "Numerical features length: 13\n"
     ]
    }
   ],
   "source": [
    "categorical_cols = ['claim_type', 'procedure_category', 'provider_specialty']\n",
    "numerical_cols = [col for col in feature_df.columns if col not in categorical_cols]\n",
    "print(f\"Categorical features length: {len(categorical_cols)}\")\n",
    "print(f\"Numerical features length: {len(numerical_cols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0b7d2750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final feature matrix: (2000, 16)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "feature_df_encoded = feature_df.copy()\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    feature_df_encoded[col] = le.fit_transform(feature_df[col].astype(str))\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(feature_df_encoded)\n",
    "print(f\"Final feature matrix: {X_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5b8a6f3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraud: 241 (12.05%)\n",
      "Normal: 1759 (87.95%)\n"
     ]
    }
   ],
   "source": [
    "y_true = df['is_fraud'].values\n",
    "print(f\"Fraud: {y_true.sum()} ({y_true.mean():.2%})\") # type: ignore\n",
    "print(f\"Normal: {(1-y_true).sum()} ({(1-y_true).mean():.2%})\") # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7b949ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduced from 16 to 10 components\n",
      "Explained variance: 96.36%\n",
      "2D PCA explained variance: 62.44%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# PCA for modeling\n",
    "pca = PCA(n_components=0.95, random_state=42)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "print(f\"Reduced from {X_scaled.shape[1]} to {X_pca.shape[1]} components\")\n",
    "print(f\"Explained variance: {pca.explained_variance_ratio_.sum():.2%}\")\n",
    "\n",
    "# PCA for 2D visualization\n",
    "pca_2d = PCA(n_components=2, random_state=42)\n",
    "X_pca_2d = pca_2d.fit_transform(X_scaled)\n",
    "print(f\"2D PCA explained variance: {pca_2d.explained_variance_ratio_.sum():.2%}\")\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30, max_iter=1000)\n",
    "X_tsne = tsne.fit_transform(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d9d3128e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e1d5344a",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_k = None\n",
    "best_silhouette = -1\n",
    "\n",
    "for k in range(2, 11):\n",
    "    kmeans_temp = KMeans(\n",
    "        n_clusters=k,\n",
    "        init=\"k-means++\",\n",
    "        n_init=10,\n",
    "        random_state=42\n",
    "    )\n",
    "    labels_temp = kmeans_temp.fit_predict(X_pca)\n",
    "    sil_score = silhouette_score(X_pca, labels_temp)\n",
    "    if sil_score > best_silhouette:\n",
    "        best_silhouette = sil_score\n",
    "        best_k = k\n",
    "\n",
    "kmeans = KMeans(\n",
    "    n_clusters=best_k,\n",
    "    init=\"k-means++\",\n",
    "    n_init=10,\n",
    "    random_state=42\n",
    ")\n",
    "kmeans_labels = kmeans.fit_predict(X_pca)\n",
    "\n",
    "kmeans_silhouette = silhouette_score(X_pca, kmeans_labels)\n",
    "kmeans_calinski = calinski_harabasz_score(X_pca, kmeans_labels)\n",
    "kmeans_davies = davies_bouldin_score(X_pca, kmeans_labels)\n",
    "\n",
    "results[\"kmeans\"] = {\n",
    "    \"labels\": kmeans_labels,\n",
    "    \"silhouette\": kmeans_silhouette,\n",
    "    \"calinski\": kmeans_calinski,\n",
    "    \"davies\": kmeans_davies,\n",
    "    \"n_clusters\": best_k,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ce09a6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "agglo = AgglomerativeClustering(\n",
    "    n_clusters=best_k,\n",
    "    linkage=\"ward\"\n",
    ")\n",
    "agglo_labels = agglo.fit_predict(X_pca)\n",
    "\n",
    "agglo_silhouette = silhouette_score(X_pca, agglo_labels)\n",
    "agglo_calinski = calinski_harabasz_score(X_pca, agglo_labels)\n",
    "agglo_davies = davies_bouldin_score(X_pca, agglo_labels)\n",
    "\n",
    "results[\"agglomerative\"] = {\n",
    "    \"labels\": agglo_labels,\n",
    "    \"silhouette\": agglo_silhouette,\n",
    "    \"calinski\": agglo_calinski,\n",
    "    \"davies\": agglo_davies,\n",
    "    \"n_clusters\": best_k,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ec43544a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dbscan = DBSCAN(eps=3.0, min_samples=10)\n",
    "dbscan_labels = dbscan.fit_predict(X_pca)\n",
    "\n",
    "n_clusters_dbscan = len(set(dbscan_labels)) - (1 if -1 in dbscan_labels else 0)\n",
    "n_noise = list(dbscan_labels).count(-1)\n",
    "\n",
    "results[\"dbscan\"] = {\n",
    "    \"labels\": dbscan_labels,\n",
    "    \"n_clusters\": n_clusters_dbscan,\n",
    "    \"n_noise\": n_noise,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1131a022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contamination rate: 0.1205\n",
      "1. Isolation Forest\n",
      "Anomalies: 241 (12.05%)\n",
      "Score range: [-0.6388, -0.3522]\n",
      "\n",
      "2. Local Outlier Factor (LOF)\n",
      "Anomalies: 241 (12.05%)\n",
      "Score range: [-2.9066, -0.9365]\n"
     ]
    }
   ],
   "source": [
    "contamination_rate = y_true.mean() # type: ignore\n",
    "print(f\"Contamination rate: {contamination_rate:.4f}\")\n",
    "\n",
    "# Isolation Forest\n",
    "print(\"1. Isolation Forest\")\n",
    "\n",
    "iso_forest = IsolationForest(contamination=contamination_rate, random_state=42, \n",
    "                             n_estimators=100, max_samples='auto')\n",
    "iso_predictions = iso_forest.fit_predict(X_pca)\n",
    "iso_scores = iso_forest.score_samples(X_pca)\n",
    "iso_predictions_binary = (iso_predictions == -1).astype(int)\n",
    "\n",
    "print(f\"Anomalies: {iso_predictions_binary.sum()} ({iso_predictions_binary.mean():.2%})\")\n",
    "print(f\"Score range: [{iso_scores.min():.4f}, {iso_scores.max():.4f}]\")\n",
    "\n",
    "results['isolation_forest'] = {\n",
    "    'predictions': iso_predictions_binary, 'scores': iso_scores,\n",
    "    'n_anomalies': iso_predictions_binary.sum()\n",
    "}\n",
    "\n",
    "# Local Outlier Factor\n",
    "print(\"\\n2. Local Outlier Factor (LOF)\")\n",
    "\n",
    "lof = LocalOutlierFactor(contamination=contamination_rate, n_neighbors=20, novelty=False)\n",
    "lof_predictions = lof.fit_predict(X_pca)\n",
    "lof_scores = lof.negative_outlier_factor_\n",
    "lof_predictions_binary = (lof_predictions == -1).astype(int)\n",
    "\n",
    "print(f\"Anomalies: {lof_predictions_binary.sum()} ({lof_predictions_binary.mean():.2%})\")\n",
    "print(f\"Score range: [{lof_scores.min():.4f}, {lof_scores.max():.4f}]\")\n",
    "\n",
    "results['lof'] = {\n",
    "    'predictions': lof_predictions_binary, 'scores': lof_scores,\n",
    "    'n_anomalies': lof_predictions_binary.sum()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ace7880b",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_results = {}\n",
    "\n",
    "def evaluate_clustering_for_fraud(cluster_labels, y_true):\n",
    "    \"\"\"Find cluster with highest fraud rate and use as anomaly cluster\"\"\"\n",
    "    unique_clusters = [c for c in np.unique(cluster_labels) if c != -1]\n",
    "    if len(unique_clusters) == 0:\n",
    "        return None\n",
    "\n",
    "    fraud_rates = {}\n",
    "    for cluster in unique_clusters:\n",
    "        mask = cluster_labels == cluster\n",
    "        fraud_rates[cluster] = y_true[mask].mean()\n",
    "\n",
    "    anomaly_cluster = max(fraud_rates, key=fraud_rates.get) # type: ignore\n",
    "    predictions = (cluster_labels == anomaly_cluster).astype(int)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "20fd2745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Model Precision Recall F1-Score ROC-AUC\n",
      "          KMEANS    0.4460 0.9253   0.6019     N/A\n",
      "   AGGLOMERATIVE    0.4451 0.9253   0.6011     N/A\n",
      "          DBSCAN    1.0000 0.0664   0.1245     N/A\n",
      "ISOLATION FOREST    0.7344 0.7344   0.7344  0.0717\n",
      "             LOF    0.0913 0.0913   0.0913  0.4421\n"
     ]
    }
   ],
   "source": [
    "# Clustering models evaluation\n",
    "for model_name in ['kmeans', 'agglomerative', 'dbscan']:\n",
    "    cluster_labels = results[model_name]['labels']\n",
    "    predictions = evaluate_clustering_for_fraud(cluster_labels, y_true)\n",
    "    \n",
    "    if predictions is not None:\n",
    "        precision = precision_score(y_true, predictions)\n",
    "        recall = recall_score(y_true, predictions)\n",
    "        f1 = f1_score(y_true, predictions)\n",
    "        cm = confusion_matrix(y_true, predictions)\n",
    "        \n",
    "        evaluation_results[model_name] = {\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'confusion_matrix': cm\n",
    "        }\n",
    "\n",
    "# Anomaly detection models evaluation\n",
    "for model_name in ['isolation_forest', 'lof']:\n",
    "    predictions = results[model_name]['predictions']\n",
    "    scores = results[model_name]['scores']\n",
    "    \n",
    "    precision = precision_score(y_true, predictions)\n",
    "    recall = recall_score(y_true, predictions)\n",
    "    f1 = f1_score(y_true, predictions)\n",
    "    cm = confusion_matrix(y_true, predictions)\n",
    "    \n",
    "    try:\n",
    "        roc_auc = roc_auc_score(y_true, -scores if model_name == 'lof' else scores)\n",
    "    except:\n",
    "        roc_auc = None\n",
    "    \n",
    "    evaluation_results[model_name] = {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'roc_auc': roc_auc,\n",
    "        'confusion_matrix': cm\n",
    "    }\n",
    "\n",
    "# Summary table\n",
    "summary_data = []\n",
    "for model_name, metrics in evaluation_results.items():\n",
    "    summary_data.append({\n",
    "        'Model': model_name.upper().replace('_', ' '),\n",
    "        'Precision': f\"{metrics['precision']:.4f}\",\n",
    "        'Recall': f\"{metrics['recall']:.4f}\",\n",
    "        'F1-Score': f\"{metrics['f1']:.4f}\",\n",
    "        'ROC-AUC': f\"{metrics['roc_auc']:.4f}\" if metrics.get('roc_auc') else 'N/A'\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(summary_df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "30b46d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "output_dir = \"E:\\\\Fraud Detection\\\\data\\\\output_data\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "27d4b06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 1: PCA Visualizations\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "\n",
    "axes[0, 0].scatter(X_pca_2d[:, 0], X_pca_2d[:, 1], c=y_true, cmap='RdYlGn_r', alpha=0.6, s=30, edgecolors='black', linewidth=0.3)\n",
    "axes[0, 0].set_title('True Fraud Labels', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('PC1')\n",
    "axes[0, 0].set_ylabel('PC2')\n",
    "\n",
    "axes[0, 1].scatter(X_pca_2d[:, 0], X_pca_2d[:, 1], c=results['kmeans']['labels'], cmap='viridis', alpha=0.6, s=30, edgecolors='black', linewidth=0.3)\n",
    "axes[0, 1].set_title('K-Means Clusters', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('PC1')\n",
    "axes[0, 1].set_ylabel('PC2')\n",
    "\n",
    "axes[0, 2].scatter(X_pca_2d[:, 0], X_pca_2d[:, 1], c=results['agglomerative']['labels'], cmap='plasma', alpha=0.6, s=30, edgecolors='black', linewidth=0.3)\n",
    "axes[0, 2].set_title('Agglomerative Clusters', fontsize=12, fontweight='bold')\n",
    "axes[0, 2].set_xlabel('PC1')\n",
    "axes[0, 2].set_ylabel('PC2')\n",
    "\n",
    "axes[1, 0].scatter(X_pca_2d[:, 0], X_pca_2d[:, 1], c=results['dbscan']['labels'], cmap='tab10', alpha=0.6, s=30, edgecolors='black', linewidth=0.3)\n",
    "axes[1, 0].set_title('DBSCAN Clusters', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('PC1')\n",
    "axes[1, 0].set_ylabel('PC2')\n",
    "\n",
    "axes[1, 1].scatter(X_pca_2d[:, 0], X_pca_2d[:, 1], c=results['isolation_forest']['predictions'], cmap='coolwarm', alpha=0.6, s=30, edgecolors='black', linewidth=0.3)\n",
    "axes[1, 1].set_title('Isolation Forest', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('PC1')\n",
    "axes[1, 1].set_ylabel('PC2')\n",
    "\n",
    "axes[1, 2].scatter(X_pca_2d[:, 0], X_pca_2d[:, 1], c=results['lof']['predictions'], cmap='coolwarm', alpha=0.6, s=30, edgecolors='black', linewidth=0.3)\n",
    "axes[1, 2].set_title('LOF', fontsize=12, fontweight='bold')\n",
    "axes[1, 2].set_xlabel('PC1')\n",
    "axes[1, 2].set_ylabel('PC2')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{output_dir}/plot1_pca_visualizations.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c49f2bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 2: t-SNE Visualizations\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
    "\n",
    "axes[0].scatter(X_tsne[:, 0], X_tsne[:, 1], c=y_true, cmap='RdYlGn_r', alpha=0.6, s=30, edgecolors='black', linewidth=0.3)\n",
    "axes[0].set_title('t-SNE: True Labels', fontsize=12, fontweight='bold')\n",
    "\n",
    "axes[1].scatter(X_tsne[:, 0], X_tsne[:, 1], c=results['kmeans']['labels'], cmap='viridis', alpha=0.6, s=30, edgecolors='black', linewidth=0.3)\n",
    "axes[1].set_title('t-SNE: K-Means', fontsize=12, fontweight='bold')\n",
    "\n",
    "axes[2].scatter(X_tsne[:, 0], X_tsne[:, 1], c=results['isolation_forest']['predictions'], cmap='coolwarm', alpha=0.6, s=30, edgecolors='black', linewidth=0.3)\n",
    "axes[2].set_title('t-SNE: Isolation Forest', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{output_dir}/plot2_tsne_visualizations.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cecb45c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 3: Anomaly Score Distributions\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "iso_scores = results['isolation_forest']['scores']\n",
    "lof_scores = results['lof']['scores']\n",
    "\n",
    "axes[0].hist(iso_scores[y_true==0], bins=50, alpha=0.6, label='Normal', color='green')\n",
    "axes[0].hist(iso_scores[y_true==1], bins=50, alpha=0.6, label='Fraud', color='red')\n",
    "axes[0].set_title('Isolation Forest Scores', fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "axes[1].hist(lof_scores[y_true==0], bins=50, alpha=0.6, label='Normal', color='green')\n",
    "axes[1].hist(lof_scores[y_true==1], bins=50, alpha=0.6, label='Fraud', color='red')\n",
    "axes[1].set_title('LOF Scores', fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{output_dir}/plot3_anomaly_scores.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d8f6155b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 4: Model Performance Comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "models = list(evaluation_results.keys())\n",
    "metrics_data = {\n",
    "    'Precision': [evaluation_results[m]['precision'] for m in models],\n",
    "    'Recall': [evaluation_results[m]['recall'] for m in models],\n",
    "    'F1-Score': [evaluation_results[m]['f1'] for m in models]\n",
    "}\n",
    "\n",
    "x = np.arange(len(models))\n",
    "width = 0.25\n",
    "\n",
    "for i, (metric, values) in enumerate(metrics_data.items()):\n",
    "    axes[0].bar(x + i*width, values, width, label=metric)\n",
    "\n",
    "axes[0].set_xlabel('Model')\n",
    "axes[0].set_ylabel('Score')\n",
    "axes[0].set_title('Model Performance Comparison', fontweight='bold')\n",
    "axes[0].set_xticks(x + width)\n",
    "axes[0].set_xticklabels([m.upper().replace('_', ' ') for m in models], rotation=45, ha='right')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3, axis='y')\n",
    "\n",
    "perf_matrix = pd.DataFrame(metrics_data, index=[m.upper().replace('_', ' ') for m in models])\n",
    "sns.heatmap(perf_matrix, annot=True, fmt='.3f', cmap='YlGnBu', ax=axes[1])\n",
    "axes[1].set_title('Performance Heatmap', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{output_dir}/plot4_performance.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2d3cdd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 5: Confusion Matrices\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (model_name, metrics) in enumerate(evaluation_results.items()):\n",
    "    cm = metrics['confusion_matrix']\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx],\n",
    "                xticklabels=['Normal', 'Fraud'], yticklabels=['Normal', 'Fraud'])\n",
    "    axes[idx].set_title(f'{model_name.upper().replace(\"_\", \" \")}', fontweight='bold')\n",
    "    axes[idx].set_xlabel('Predicted')\n",
    "    axes[idx].set_ylabel('True')\n",
    "\n",
    "for i in range(len(evaluation_results), 6):\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{output_dir}/plot5_confusion_matrices.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b3abce26",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Figure 6: Cluster Analysis\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "cluster_data = []\n",
    "for model_name in ['kmeans', 'agglomerative', 'dbscan']:\n",
    "    labels = results[model_name]['labels']\n",
    "    for label in np.unique(labels):\n",
    "        if label != -1:\n",
    "            mask = labels == label\n",
    "            cluster_data.append({\n",
    "                'Model': model_name.upper(),\n",
    "                'Cluster': f\"{model_name.upper()}-C{label}\",\n",
    "                'Size': mask.sum(),\n",
    "                'Fraud_Count': y_true[mask].sum(),\n",
    "                'Fraud_Rate': y_true[mask].mean() * 100,\n",
    "                'Avg_Amount': df.loc[mask, 'claim_amount'].mean()\n",
    "            })\n",
    "\n",
    "cluster_df = pd.DataFrame(cluster_data)\n",
    "\n",
    "cluster_df.plot(x='Cluster', y='Fraud_Rate', kind='bar', ax=axes[0,0], legend=False, color='red', alpha=0.7)\n",
    "axes[0,0].set_title('Fraud Rate by Cluster (%)', fontweight='bold')\n",
    "axes[0,0].set_ylabel('Fraud Rate (%)')\n",
    "axes[0,0].grid(alpha=0.3, axis='y')\n",
    "\n",
    "cluster_df.plot(x='Cluster', y='Size', kind='bar', ax=axes[0,1], legend=False, color='blue', alpha=0.7)\n",
    "axes[0,1].set_title('Cluster Size', fontweight='bold')\n",
    "axes[0,1].set_ylabel('Number of Claims')\n",
    "axes[0,1].grid(alpha=0.3, axis='y')\n",
    "\n",
    "cluster_df.plot(x='Cluster', y='Avg_Amount', kind='bar', ax=axes[1,0], legend=False, color='green', alpha=0.7)\n",
    "axes[1,0].set_title('Average Claim Amount by Cluster', fontweight='bold')\n",
    "axes[1,0].set_ylabel('Average Amount ($)')\n",
    "axes[1,0].grid(alpha=0.3, axis='y')\n",
    "\n",
    "cluster_df.plot(x='Cluster', y='Fraud_Count', kind='bar', ax=axes[1,1], legend=False, color='orange', alpha=0.7)\n",
    "axes[1,1].set_title('Fraud Count by Cluster', fontweight='bold')\n",
    "axes[1,1].set_ylabel('Number of Frauds')\n",
    "axes[1,1].grid(alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{output_dir}/plot6_cluster_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7ec78851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 50 anomalies saved:\n",
      "  - E:\\Fraud Detection\\data\\output_data/top50_isolation_forest.csv\n",
      "  - E:\\Fraud Detection\\data\\output_data/top50_lof.csv\n"
     ]
    }
   ],
   "source": [
    "# Extract anomaly scores\n",
    "iso_scores = results['isolation_forest']['scores']\n",
    "lof_scores = results['lof']['scores']\n",
    "\n",
    "# Get top 50 most suspicious claims (lowest scores = most anomalous)\n",
    "top50_iso_idx = np.argsort(iso_scores)[:50]\n",
    "top50_lof_idx = np.argsort(lof_scores)[:50]\n",
    "\n",
    "# Create report for Isolation Forest top 50\n",
    "iso_report = df.iloc[top50_iso_idx].copy()\n",
    "iso_report['anomaly_score'] = iso_scores[top50_iso_idx]\n",
    "iso_report['rank'] = range(1, 51)\n",
    "iso_report = iso_report[['rank', 'claim_id', 'claim_amount', 'claim_type', 'member_age', \n",
    "                         'num_procedures', 'length_of_stay_days', 'days_since_policy_start',\n",
    "                         'multiple_claims_same_day', 'anomaly_score', 'is_fraud']]\n",
    "\n",
    "iso_report.to_csv(f'{output_dir}/top50_isolation_forest.csv', index=False)\n",
    "\n",
    "# Create report for LOF top 50\n",
    "lof_report = df.iloc[top50_lof_idx].copy()\n",
    "lof_report['anomaly_score'] = lof_scores[top50_lof_idx]\n",
    "lof_report['rank'] = range(1, 51)\n",
    "lof_report = lof_report[['rank', 'claim_id', 'claim_amount', 'claim_type', 'member_age',\n",
    "                         'num_procedures', 'length_of_stay_days', 'days_since_policy_start',\n",
    "                         'multiple_claims_same_day', 'anomaly_score', 'is_fraud']]\n",
    "\n",
    "lof_report.to_csv(f'{output_dir}/top50_lof.csv', index=False)\n",
    "\n",
    "print(f\"Top 50 anomalies saved:\")\n",
    "print(f\"  - {output_dir}/top50_isolation_forest.csv\")\n",
    "print(f\"  - {output_dir}/top50_lof.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "30425147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "output_dir = \"E:\\\\Fraud Detection\\\\models\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save scaler\n",
    "with open(f'{output_dir}/scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "# Save PCA\n",
    "with open(f'{output_dir}/pca.pkl', 'wb') as f:\n",
    "    pickle.dump(pca, f)\n",
    "\n",
    "# Save label encoders\n",
    "label_encoders = {}\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    le.fit(df[col].astype(str))\n",
    "    label_encoders[col] = le\n",
    "\n",
    "with open(f'{output_dir}/label_encoders.pkl', 'wb') as f:\n",
    "    pickle.dump(label_encoders, f)\n",
    "\n",
    "# Get the Isolation Forest model from your results dict\n",
    "iso_model = IsolationForest(contamination=0.12, random_state=42, n_estimators=100)\n",
    "iso_model.fit(X_pca)\n",
    "\n",
    "with open(f'{output_dir}/isolation_forest.pkl', 'wb') as f:\n",
    "    pickle.dump(iso_model, f)\n",
    "\n",
    "# For LOF, need to use novelty=True for new predictions\n",
    "lof_for_pred = LocalOutlierFactor(contamination=0.12, novelty=True)\n",
    "lof_for_pred.fit(X_pca)\n",
    "\n",
    "with open(f'{output_dir}/lof.pkl', 'wb') as f:\n",
    "    pickle.dump(lof_for_pred, f)\n",
    "\n",
    "\n",
    "print(\"Models saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c33653a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Fraud Detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
